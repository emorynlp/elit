{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEsqbG3EcLTV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/emorynlp/elit/blob/dev/docs/tutorial.ipynb)\n",
    "\n",
    "ELIT can be installed using pip, though it's not officially on PyPi yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U git+https://github.com/emorynlp/elit.git@dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The common workflow for ELIT is to load a model then call it as a function. Models in ELIT are represented as string typed indentifiers which are grouped by tasks. For example, let's list all the models in ELIT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import elit\n",
    "elit.pretrained.ALL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all the MultiTaskLearning models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "elit.pretrained.mtl.ALL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELIT offers several models for the same task with different settings. For example, the `LEM_POS_NER_DEP_SDP_CON_AMR_ROBERTA_BASE_EN` model is finetuned with RoBERTa-base. Let's load it and see what it can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mtl = elit.load(elit.pretrained.mtl.LEM_POS_NER_DEP_SDP_CON_AMR_ROBERTA_BASE_EN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you call `load` on a model, ELIT will download it and load it into the main memory or the GPU if you have one. The loaded model behaves just like a function which you can pass in a list of tokenized sentences as arguments and get the annotations as the returned value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "doc = mtl([\n",
    "     [\"Emory\", \"NLP\", \"is\", \"a\", \"research\", \"lab\", \"in\", \"Atlanta\", \".\"],\n",
    "     [\"It\", \"is\", \"founded\", \"by\", \"Jinho\", \"D.\", \"Choi\", \"in\", \"2014\", \".\", \"Dr.\", \"Choi\", \"is\", \"a\", \"professor\", \"at\", \"Emory\", \"University\", \".\"]\n",
    "])\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As you can see, the returned `doc` is a Python dict storing outputs from different models. Refer to our GitHub docs for its format and guidelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M2kjBfKJVV4i",
    "outputId": "0d14eabf-b0e2-4614-cdba-2c97e21b3792",
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "As you can see, the returned `doc` is a Python dict storing outputs from different models. Refer to our GitHub docs for its format and guidelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFWxRPEBc6PQ"
   },
   "source": [
    "The common workflow for ELIT is to load a model then call it as a function. Models in ELIT are represented as string typed indentifiers which are grouped by tasks. For example, let's list all the models in ELIT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pwOpIwPEiI-S",
    "outputId": "a1de06c1-863c-4177-aa88-adb7e26590bb",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import elit\n",
    "elit.pretrained.ALL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3h3e3hLi-hE"
   },
   "source": [
    "List all the MultiTaskLearning models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GFQBnfsyjHKr",
    "outputId": "aea65f4c-0c93-4bf6-e821-2c7d43823dc2",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "elit.pretrained.mtl.ALL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ct8nCfVAjNWV"
   },
   "source": [
    "ELIT offers several models for the same task with different settings. For example, the `LEM_POS_NER_DEP_SDP_CON_AMR_ROBERTA_BASE_EN` model is finetuned with RoBERTa-base. Let's load it and see what it can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477,
     "referenced_widgets": [
      "f1f828817c8d4ca388b4e042fdd1fc01",
      "f059614b4f7c4a8091d11b9f47e3bc65",
      "e97ed112c6514d338c52dd6336d90846",
      "65cea109d54b4107bc14e067975c10cc",
      "fe2d3c2b85954622ae7e89aeaab17aec",
      "d9e9aeb4335048b29d25637e1c546299",
      "a617a1c973a64fd186c8e5d2c9a0c07f",
      "ddf835fef52347dc9da373ee0a4fd06d",
      "812879fb4f5f4e7e8c7423f17e3c791f",
      "85db0313c6d64c0e8a988b21e2989636",
      "cb4028b0a529489c8999acd7189f751c",
      "2d31f00b0f8c49758734777c251781c0",
      "b8301e17364d4fc88b372e4f5fc92c62",
      "1cde4e5a80614ac3bd0b7a725a1cf55f",
      "083d8838c9e747fe84851b70bc9f8dd6",
      "4807dbb9f9e7496e9c279768c684f35b",
      "60fbc730923d490bb1ff201467f3e77c",
      "483fff57b9674b21b306916e999a00b5",
      "ce8025139f304b68ac74f1adae5c732b",
      "a450d122e4624c5396d45faecc8d0669",
      "ade6a90971f0460eb96f12cd27f75041",
      "6d7b59f8f5414bea91f1b54666d088e1",
      "6dfef9ae28e549e591042ccc198ae056",
      "233f25e2c4e344fda8b14043a7538ba4",
      "5b6938cacc174a038841a5e67fad27c5",
      "335d58e158ff44319f4cb9829bafbad9",
      "743fee8fbfc74214a724d560a0df7ff7",
      "2648ec6ff34541cfa90eaebd194fe71d",
      "30dfe49443bd403c97eab5651139a50b",
      "6abde990df3242088902cffe12dc00a0",
      "8e2cd821c7634bb398f12baa71c8246c",
      "aaf3d9618cb04a76bb3f676f510e8e9d"
     ]
    },
    "id": "vnjYWKSPj2aX",
    "outputId": "7f3be6d2-4a84-4a8c-90f1-ee3295ddc50f",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "mtl = elit.load(elit.pretrained.mtl.LEM_POS_NER_DEP_SDP_CON_AMR_ROBERTA_BASE_EN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GT6lULzuj_uP"
   },
   "source": [
    "Once you call `load` on a model, ELIT will download it and load it into the main memory or the GPU if you have one. The loaded model behaves just like a function which you can pass in a list of tokenized sentences as arguments and get the annotations as the returned value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ecrBs0xkjxG",
    "outputId": "6357f0b2-98c5-4cef-de00-a2faa0bcc9d7",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "doc = mtl([\n",
    "     [\"Emory\", \"NLP\", \"is\", \"a\", \"research\", \"lab\", \"in\", \"Atlanta\", \".\"],\n",
    "     [\"It\", \"is\", \"founded\", \"by\", \"Jinho\", \"D.\", \"Choi\", \"in\", \"2014\", \".\", \"Dr.\", \"Choi\", \"is\", \"a\", \"professor\", \"at\", \"Emory\", \"University\", \".\"]\n",
    "])\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyckra2slA0v"
   },
   "source": [
    "As you can see, the returned `doc` is a Python dict storing outputs from different models. Refer to our GitHub docs for its format and guidelines."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "elit.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}